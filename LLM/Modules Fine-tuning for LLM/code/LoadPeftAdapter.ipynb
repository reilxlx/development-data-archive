{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "226ec326-9169-47bf-971b-c0b61e066f4e",
   "metadata": {},
   "source": [
    "PEFT (Parameter-Efficient Fine-Tuning) 的工作原理是在预训练模型的基础上，冻结大部分参数，只微调少量新增的参数。PEFT 模型本身并不包含完整的模型结构和参数，它只保存了微调的部分。\n",
    "因此，要使用 PEFT 模型，必须先加载预训练的主模型，然后将 PEFT 模块加载到主模型上，才能进行推理或训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7fe66f-1bb7-4e99-b301-9c27fab61ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Using {torch.cuda.device_count()} GPU(s).\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\") \n",
    "\n",
    "def print_lora_details(model, layer_idx=0, module_name=\"v_proj\"):\n",
    "    original_weight = eval(f\"model.model.model.layers[{layer_idx}].self_attn.{module_name}.weight\")\n",
    "    lora_A = eval(f\"model.model.model.layers[{layer_idx}].self_attn.{module_name}.lora_A.default.weight\")\n",
    "    lora_B = eval(f\"model.model.model.layers[{layer_idx}].self_attn.{module_name}.lora_B.default.weight\")\n",
    "\n",
    "    print(f\"Layer {layer_idx} '{module_name}' details:\")\n",
    "    print(f\"Original weight:\\n{original_weight}\")\n",
    "    print(f\"LoRA A:\\n{lora_A}\")\n",
    "    print(f\"LoRA B:\\n{lora_B}\")\n",
    "\n",
    "peft_model_id = \"/root/datasetsSplitTrain/results/en_dataset_q_proj/\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "base_model_name = config.base_model_name_or_path  \n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name,\n",
    "    device_map='cuda:0',\n",
    "    torch_dtype=torch.bfloat16)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, peft_model_id, torch_dtype=torch.bfloat16)\n",
    "\n",
    "target_layer_number = 0\n",
    "\n",
    "print_lora_details(peft_model, layer_idx=0, module_name=\"q_proj\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
