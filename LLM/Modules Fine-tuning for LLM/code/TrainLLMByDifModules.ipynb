{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc56048-e050-424e-b94a-aed82dcad736",
   "metadata": {},
   "source": [
    "### 核心想法：\n",
    "例如对meta-llama/Meta-Llama-3-8B-Instruct进行微调，一般情况下有以下模块可以成为微调模型的选择:\n",
    "```bash\n",
    "\"target_modules\": [\n",
    "    \"gate_proj\",\n",
    "    \"down_proj\",\n",
    "    \"up_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"q_proj\"\n",
    "  ]\n",
    "```\n",
    "当我们进行模型的SFT微调，一般情况选择Lora或者QLora方式，在有以上不同target_modules模块选择的情况下，是否可以对不同的SFT数据集选择不同的target_modules进行微调。</br>\n",
    "例如：对中文微调数据集选择q_proj，对英文微调数据集选择v_proj，对日语微调数据集选择v_proj...等等此类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cdb25d-837b-4d11-ace1-44a2ba2d89d2",
   "metadata": {},
   "source": [
    "model.merge_and_unload() 的作用是将 LoRA 训练得到的参数 融合 到原始模型的参数中，而不是直接改变模型的结构。\n",
    "\n",
    "LoRA 训练阶段:\n",
    "LoRA 并没有直接训练原始模型的参数。相反，它在需要微调的层 (比如 Transformer 中的线性层) 上引入了新的、更小的可训练矩阵 (A 和 B)。\n",
    "在训练过程中，只有 LoRA 的 A 和 B 矩阵的参数会被更新，而原始模型的参数保持冻结。\n",
    "由于 A 和 B 矩阵的秩远小于原始权重矩阵，因此 LoRA 可以显著减少需要训练的参数量。\n",
    "model.merge_and_unload() 操作:\n",
    "\n",
    "这个操作的核心是将 LoRA 训练得到的 A 和 B 矩阵的低秩分解结果应用到原始模型的权重矩阵上。\n",
    "具体来说，它会计算 W_new = W_original + B @ A，并将 W_new 作为新的权重矩阵直接更新到原始模型的对应层中。\n",
    "完成这个操作后，LoRA 的 A 和 B 矩阵会被从内存中卸载，因为它们的信息已经被融合到原始模型的参数中了。\n",
    "因此，model.merge_and_unload() 虽然改变了原始模型的参数值，但并没有改变模型的结构或层名。 模型的结构定义 (比如哪些层连接在一起) 以及层的名称在整个过程中都保持不变。\n",
    "\n",
    "你可以把 LoRA 想象成一种 \"插件式\" 的微调方法：它在训练时像 \"插件\" 一样附加到原始模型上，但在最终合并后，它的效果会被 \"融入\" 到原始模型中，而 \"插件\" 本身会被移除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef1dddb-e2b7-48b0-a6b1-8239988d8be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.40.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30447e60-ab86-4204-adc3-3008c45b681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import Union, List\n",
    "import datasets\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import transformers\n",
    "\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "logger = logging.getLogger('__name__')\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"You are a helpful assistant. 你是一个乐于助人的助手。\"\"\"\n",
    "system_format='<|start_header_id|>system<|end_header_id|>\\n\\n{content}<|eot_id|>'\n",
    "user_format='<|start_header_id|>user<|end_header_id|>\\n\\n{content}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
    "assistant_format='{content}<|eot_id|>'\n",
    "\n",
    "def build_instruction_dataset(data_path: Union[List[str],str],\n",
    "                tokenizer: transformers.PreTrainedTokenizer,\n",
    "                max_seq_length: int, data_cache_dir = None,\n",
    "                preprocessing_num_workers = None):\n",
    "\n",
    "    def tokenization(examples):\n",
    "        sources = []\n",
    "        targets = []\n",
    "        for instruction, input_text, output in zip(examples['instruction'],examples['input'],examples['output']):\n",
    "            if input_text is not None and input_text !=\"\":\n",
    "                instruction = instruction+'\\n'+input_text\n",
    "            source = system_format.format(content=DEFAULT_SYSTEM_PROMPT) + user_format.format(content=instruction)\n",
    "            target = assistant_format.format(content=output)\n",
    "\n",
    "            sources.append(source)\n",
    "            targets.append(target)\n",
    "\n",
    "        tokenized_sources = tokenizer(sources, return_attention_mask=False)\n",
    "        tokenized_targets = tokenizer(targets, return_attention_mask=False)\n",
    "\n",
    "        all_input_ids = []\n",
    "        all_labels = []\n",
    "        for s,t in zip(tokenized_sources['input_ids'],tokenized_targets['input_ids']):\n",
    "            input_ids = torch.LongTensor(s + t)[:max_seq_length]\n",
    "            labels = torch.LongTensor([IGNORE_INDEX] * len(s) + t)[:max_seq_length]\n",
    "            all_input_ids.append(input_ids)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "        results = {'input_ids':all_input_ids, 'labels': all_labels}\n",
    "        return results\n",
    "\n",
    "\n",
    "    logging.warning(\"building dataset...\")\n",
    "    all_datasets = []\n",
    "\n",
    "    if not isinstance(data_path,(list,tuple)):\n",
    "        data_path = [data_path]\n",
    "    for file in data_path:\n",
    "\n",
    "        if data_cache_dir is None:\n",
    "            data_cache_dir = str(os.path.dirname(file))\n",
    "        cache_path = os.path.join(data_cache_dir,os.path.basename(file).split('.')[0]+f\"_{max_seq_length}\")\n",
    "        os.makedirs(cache_path, exist_ok=True)\n",
    "        try:\n",
    "            processed_dataset = datasets.load_from_disk(cache_path)\n",
    "            logger.info(f'training datasets-{file} has been loaded from disk')\n",
    "        except Exception:\n",
    "            raw_dataset = load_dataset(\"json\", data_files=file, cache_dir=cache_path)\n",
    "            tokenization_func = tokenization\n",
    "            tokenized_dataset = raw_dataset.map(\n",
    "                tokenization_func,\n",
    "                batched=True,\n",
    "                num_proc=preprocessing_num_workers,\n",
    "                remove_columns=[\"instruction\",\"input\",\"output\"],\n",
    "                keep_in_memory=False,\n",
    "                desc=\"preprocessing on dataset\",\n",
    "            )\n",
    "            processed_dataset = tokenized_dataset\n",
    "            processed_dataset.save_to_disk(cache_path)\n",
    "        processed_dataset.set_format('torch')\n",
    "        all_datasets.append(processed_dataset['train'])\n",
    "    all_datasets = concatenate_datasets(all_datasets)\n",
    "    return all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aea55f-3c9d-40b9-b9b1-81ae4726b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c76018-e1bb-427e-b9ae-ff3a23f36461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c47ff7-5714-4cb7-b98c-0f662f430c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a00d8-1de3-4897-8f21-dba095076bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Using {torch.cuda.device_count()} GPU(s).\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")  # 应该输出 0\n",
    "else:\n",
    "    print(\"CUDA is not available.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe1542-b78b-47e2-ab85-d7abbc2d0d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(torch.device('cuda:0'))\n",
    "model_name = \"/root/llama3/mode/Meta-Llama-3-8B-Instruct/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='cuda:0',\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d95ee33-0050-4441-afda-d13442760c00",
   "metadata": {},
   "source": [
    "你提出的问题很好！虽然在加载模型时使用 `torch.bfloat16` 指定了模型参数和计算的精度，但 **通常不需要** 为 tokenizer 也指定 `torch_dtype`。 \n",
    "\n",
    "原因如下：\n",
    "\n",
    "1. **Tokenizer 的数据类型:** Tokenizer 主要处理文本数据，例如将文本转换为 token ID，以及将 token ID 转换回文本。这些操作通常使用整数类型（如 `int32`）来表示 token ID，而不是浮点数类型。\n",
    "\n",
    "2. **Tokenizer 的显存占用:**  Tokenizer 本身的显存占用相对较小，因为它主要存储词汇表和一些预处理规则。相比之下，模型参数和计算过程占用了大部分显存。\n",
    "\n",
    "3. **框架的默认行为:**  像 Transformers 库中的 `AutoTokenizer` 通常会根据模型的配置自动选择合适的数据类型。 \n",
    "\n",
    "**总结:**\n",
    "\n",
    "* 你目前的代码已经很好地将模型加载到了 `bfloat16` 精度，这将有效减少模型的显存占用。\n",
    "* 不需要为 tokenizer 指定 `torch_dtype`，因为它不会显著影响显存使用。\n",
    "\n",
    "**其他减少显存占用的方法:**\n",
    "\n",
    "* **梯度累积 (Gradient Accumulation):**  将多个 mini-batch 的梯度累积起来，再进行一次参数更新，可以模拟更大的 batch size，从而减少显存占用。\n",
    "* **混合精度训练 (Mixed Precision Training):**  在训练过程中，使用 `float16` 进行计算，同时使用 `float32` 存储模型参数，可以加速训练并减少显存占用。\n",
    "* **模型量化 (Model Quantization):**  将模型参数和激活值从高精度浮点数转换为低精度整数，可以显著减少模型大小和显存占用。\n",
    "\n",
    "希望这些信息能够帮助你！ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bda303-1ea4-4fed-8b97-8c451386fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    print(\"pad_token is None\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9bc5a2-c8dd-4c01-9839-4a6808aa7f07",
   "metadata": {},
   "source": [
    "这段代码的作用是：**如果 tokenizer 没有设置 pad token，则将 pad token 设置为 eos token。**\n",
    "\n",
    "让我们逐步解释：\n",
    "\n",
    "1. **`tokenizer.pad_token`**: \n",
    "   -  在自然语言处理中，模型通常需要处理长度不一的文本序列。为了方便模型处理，我们会将所有序列填充到相同的长度，而填充的部分就使用 `pad_token` 来表示。\n",
    "   -  `tokenizer.pad_token` 表示 tokenizer 用来进行填充的特殊 token。\n",
    "\n",
    "2. **`tokenizer.eos_token`**: \n",
    "   -  `eos_token`  代表 \"end of sentence\"，即句子结束标记符。它通常用于标记一个句子的结束。\n",
    "\n",
    "3. **`if tokenizer.pad_token is None:`**: \n",
    "   -  这行代码首先检查 tokenizer 是否已经设置了 `pad_token`。 \n",
    "\n",
    "4. **`tokenizer.pad_token = tokenizer.eos_token`**: \n",
    "   -  如果 `tokenizer.pad_token` 为 `None`，说明 tokenizer 还没有设置 `pad_token`，那么就将 `tokenizer.eos_token` 赋给 `tokenizer.pad_token`。\n",
    "\n",
    "**为什么要这样做？**\n",
    "\n",
    "- 一些预训练模型的 tokenizer 可能没有默认设置 `pad_token`，但通常会有 `eos_token`。\n",
    "- 在很多情况下，将 `pad_token` 设置为 `eos_token` 是合理的，因为填充的部分通常出现在序列的末尾，语义上类似于句子结束。\n",
    "\n",
    "**总结:**\n",
    "\n",
    "这段代码确保了 tokenizer 有一个有效的 `pad_token`，以便在处理变长序列时进行填充操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b15d9-0d00-42cf-bf14-cc157d2c34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_targets = {\n",
    "    \"en_dataset\": {\n",
    "        \"target_modules\": [\"q_proj\"],\n",
    "        \"lora_config\": LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "        )\n",
    "    },\n",
    "    \"zh_dataset\": {\n",
    "        \"target_modules\": [\"v_proj\"],\n",
    "        \"lora_config\": LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "        )\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642c7d44-e2d4-4620-bdda-0393f42a70a4",
   "metadata": {},
   "source": [
    "在提供的配置中，`\"task_type\": \"CAUSAL_LM\"`  表示你正在使用 **PEFT (Parameter-Efficient Fine-Tuning)** 库对一个 **因果语言模型 (Causal Language Model)** 进行微调。 \n",
    "\n",
    "让我们分别解释这两个部分:\n",
    "\n",
    "* **PEFT (Parameter-Efficient Fine-Tuning):**  PEFT 是一种技术，用于在微调大型语言模型时减少需要更新的参数数量。这使得微调过程更高效，并且可以减少对计算资源的需求。常见的 PEFT 方法包括 LoRA (Low-Rank Adaptation), Prefix Tuning, Adapter Tuning 等。\n",
    "\n",
    "* **CAUSAL_LM (Causal Language Model):**  因果语言模型是一种语言模型，它学习预测给定上下文中的下一个词。这种模型也被称为自回归语言模型，因为它的预测只依赖于前面的词。 GPT (Generative Pre-trained Transformer) 系列模型就是因果语言模型的典型例子。\n",
    "\n",
    "**总结:**\n",
    "\n",
    "`\"task_type\": \"CAUSAL_LM\"`  告诉 PEFT 库你正在微调一个因果语言模型，它会根据这个信息选择合适的优化器、损失函数以及其他相关设置。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a380834-465f-49a4-a7a8-f59ca03c43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = { \n",
    "    \"en_dataset\": build_instruction_dataset(\n",
    "                data_path='/root/llama3/code/datasetsSplitTrain/tigerbot-wiki-qa-zh-1k.jsonl',\n",
    "                tokenizer=tokenizer,\n",
    "                max_seq_length=1024,\n",
    "                data_cache_dir='./datacache',\n",
    "                preprocessing_num_workers=8\n",
    "    ),\n",
    "    \"zh_dataset\": build_instruction_dataset(\n",
    "                data_path='/root/llama3/code/datasetsSplitTrain/tigerbot-riddle-qa-1k.jsonl',\n",
    "                tokenizer=tokenizer,\n",
    "                max_seq_length=1024,\n",
    "                data_cache_dir='./datacache',\n",
    "                preprocessing_num_workers=8\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1819ba5-37a0-434b-8a7e-1520ed43bb9a",
   "metadata": {},
   "source": [
    "`max_seq_length` 参数决定了模型能够处理的最长序列长度，它对模型训练和性能有重要影响。选择合适的 `max_seq_length` 需要权衡以下因素：\n",
    "\n",
    "**1. 数据集特性:**\n",
    "\n",
    "* **序列长度分布:** 首先要分析你的英文和中文数据集 (`tigerbot-wiki-qa-zh-100.jsonl` 和 `tigerbot-riddle-qa-100.jsonl`) 中序列长度的分布情况。如果大多数序列长度都在 2048 以内，那么设置 `max_seq_length=2048` 是合理的。如果存在大量超过 2048 的序列，则需要考虑增加 `max_seq_length`。\n",
    "* **任务类型:**  不同的任务对序列长度的要求不同。例如，问答任务通常需要较长的序列长度来容纳问题和答案，而文本分类任务则可能只需要较短的序列长度。\n",
    "\n",
    "**2. 模型特性:**\n",
    "\n",
    "* **模型架构:**  不同的模型架构对序列长度的限制不同。例如，基于 Transformer 的模型通常能够处理更长的序列。\n",
    "* **模型大小:**  更大的模型通常能够处理更长的序列，但也需要更多的计算资源。\n",
    "\n",
    "**3. 计算资源:**\n",
    "\n",
    "* **显存限制:**  `max_seq_length` 越大，模型训练和推理所需的显存就越多。如果你的计算资源有限，就需要限制 `max_seq_length`。\n",
    "* **训练时间:**  `max_seq_length` 越大，模型训练所需的时间就越长。\n",
    "\n",
    "**建议:**\n",
    "\n",
    "1. **分析数据集:** 使用工具 (例如 pandas, matplotlib) 分析数据集中的序列长度分布，找到一个能够覆盖大部分样本的 `max_seq_length` 值。\n",
    "2. **逐步尝试:**  从一个较小的 `max_seq_length` 值开始 (例如 2048)，逐步增加，观察模型性能和训练时间变化，找到一个最佳平衡点。\n",
    "3. **考虑资源限制:**  根据你的计算资源 (例如 GPU 显存) 限制 `max_seq_length`，避免出现内存不足错误。\n",
    "\n",
    "**总结:**\n",
    "\n",
    "`max_seq_length` 的选择需要综合考虑数据集、模型和计算资源等因素。建议进行实验，找到一个既能满足任务需求，又能高效利用资源的最优值。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80ef6f-7f8c-45b3-9d46-d1b74894c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_attn_0_q_proj_weights = model.model.layers[0].self_attn.q_proj.weight\n",
    "print(\"原始模型q_proj参数：\")\n",
    "print(original_model_attn_0_q_proj_weights)\n",
    "original_model_attn_0_v_proj_weights = model.model.layers[0].self_attn.v_proj.weight\n",
    "print(\"原始模型v_proj参数：\")\n",
    "print(original_model_attn_0_v_proj_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9162482-f1da-4963-afb1-ce066647d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e714a16-f8b8-49ba-a5f6-6f3c6b03df7c",
   "metadata": {},
   "source": [
    "你观察到的现象是由于 `prepare_model_for_kbit_training(model)` 函数对模型进行了封装和修改，导致参数的属性发生了一些变化。\n",
    "\n",
    "**原因分析:**\n",
    "\n",
    "1. **模型封装:**  `prepare_model_for_kbit_training(model)` 函数通常会使用 k-bit 训练库（例如 `bitsandbytes`）提供的类或函数对原始模型进行封装。这意味着你访问到的 `model.model.layers[0].self_attn.q_proj.weight`  实际上已经是封装后的模型的参数，而不是原始模型的参数。\n",
    "\n",
    "2. **参数类型转换:** 在 k-bit 训练中，为了实现量化，模型参数的类型可能会被转换为 k-bit 训练库所使用的特定类型。例如，`bitsandbytes` 库会将模型参数转换为 `torch.cuda.HalfTensor` 类型（即 FP16）或自定义的量化类型。\n",
    "\n",
    "3. **属性隐藏:**  封装后的模型可能会隐藏一些原始参数的属性，例如 `dtype` 和 `requires_grad`，以简化用户接口或避免混淆。这是因为 k-bit 训练库通常会自动处理这些属性，用户无需直接操作。\n",
    "\n",
    "**解释:**\n",
    "\n",
    "-  `device='cuda:0'` 属性仍然存在，因为 k-bit 训练通常也在 GPU 上进行，所以模型参数仍然需要位于 GPU 设备上。\n",
    "\n",
    "-  `dtype=torch.bfloat16, requires_grad=True` 属性消失，是因为 k-bit 训练库可能已经将参数类型转换为其他类型，并且自动处理了梯度计算。\n",
    "\n",
    "**总结:**\n",
    "\n",
    "`prepare_model_for_kbit_training(model)` 函数对模型进行了封装和修改，导致参数的属性发生了一些变化。这是为了实现 k-bit 训练的量化和优化，用户通常不需要直接操作这些属性。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d380d-cb58-408d-8d89-ba2ce620b604",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_model_for_kbit_training_count = sum(p.numel() for p in model.parameters())\n",
    "print(f\"prepare_model_for_kbit_training_count': {prepare_model_for_kbit_training_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a9a12-7124-46a1-a9b6-41ccdd7d2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lora_details(model, layer_idx=0, module_name=\"v_proj\"):\n",
    "    original_weight = eval(f\"model.model.model.layers[{layer_idx}].self_attn.{module_name}.weight\")\n",
    "    lora_A = eval(f\"model.model.model.layers[{layer_idx}].self_attn.{module_name}.lora_A.default.weight\")\n",
    "    lora_B = eval(f\"model.model.model.layers[{layer_idx}].self_attn.{module_name}.lora_B.default.weight\")\n",
    "    # 计算 W_new = W_original + B @ A\n",
    "    lora_B_A = torch.matmul(lora_B, lora_A)\n",
    "    computed_merged_weight = original_weight + lora_B_A\n",
    "\n",
    "    print(f\"Layer {layer_idx} '{module_name}' details:\")\n",
    "    print(f\"Original weight:\\n{original_weight}\")\n",
    "    print(f\"LoRA A:\\n{lora_A}\")\n",
    "    print(f\"LoRA B:\\n{lora_B}\")\n",
    "    print(f\"Lora_B_A:\\n{lora_B_A}\")\n",
    "    print(f\"Computed merged weight (W_original + B @ A):\\n{computed_merged_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c84ec-0392-470e-9d71-194185e4078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_merged_details(model, layer_idx=0, module_name=\"v_proj\"):\n",
    "    merged_weight = eval(f\"model.model.model.layers[{layer_idx}].self_attn.{module_name}.weight\")\n",
    "    print(f\"Merged model weight:\\n{merged_weight}\")\n",
    "\n",
    "def get_model_parameter(model, layer_id, proj_type):\n",
    "  param_str = f\"model.model.model.layers[{layer_id}].self_attn.{proj_type}.weight\"\n",
    "  param = eval(param_str)\n",
    "  print(f\"Shape of {proj_type} in layer {layer_id}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca77c4-3a92-40f5-bc93-e8c52379ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, tuning_info in tuning_targets.items(): \n",
    "    lora_config = tuning_info[\"lora_config\"]\n",
    "    lora_config.target_modules = tuning_info[\"target_modules\"]\n",
    "\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "    total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "    lora_params = 0\n",
    "    for n, p in peft_model.named_parameters():\n",
    "        if \"lora\" in n:\n",
    "            lora_params += p.numel()\n",
    "    print(f\"LoRA parameters: {lora_params}\")\n",
    "\n",
    "    original_params = total_params - lora_params\n",
    "    print(f\"Original model parameters: {original_params}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    target_module_str = '_'.join(tuning_info['target_modules'])\n",
    "    output_dir = f\"./results/{dataset_name}_{target_module_str}\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,  \n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=1, \n",
    "        gradient_accumulation_steps=1, \n",
    "        learning_rate=2e-5,\n",
    "        warmup_ratio=0.05,\n",
    "        bf16=True,\n",
    "        logging_steps=100,\n",
    "        save_steps=500\n",
    "    )\n",
    "    \n",
    "    print(f\"Training on {dataset_name} with target modules: {tuning_info['target_modules']}\") \n",
    "    trainer = Trainer( \n",
    "        model=peft_model, \n",
    "        args=training_args, \n",
    "        train_dataset=datasets[dataset_name], \n",
    "        tokenizer=tokenizer \n",
    "    ) \n",
    "    trainer.train()\n",
    "    \n",
    "    peft_model.save_pretrained(output_dir, torch_dtype=torch.bfloat16)\n",
    "    \n",
    "    original_param_count = sum(p.numel() for p in peft_model.parameters())\n",
    "    print(f\"original_param_count': {original_param_count}\")\n",
    "\n",
    "    if target_module_str == \"q_proj\":\n",
    "        get_model_parameter(peft_model, 0, 'q_proj')\n",
    "        print(\"Before merging:\")\n",
    "        print_lora_details(peft_model, layer_idx=0, module_name=\"q_proj\")\n",
    "    else:\n",
    "        get_model_parameter(peft_model, 0, 'v_proj')\n",
    "        print(\"Before merging:\")\n",
    "        print_lora_details(peft_model, layer_idx=0, module_name=\"v_proj\")\n",
    "    \n",
    "    peft_model.merge_and_unload()\n",
    "    \n",
    "    if target_module_str == \"q_proj\":\n",
    "        get_model_parameter(peft_model, 0, 'q_proj')\n",
    "        print_merged_details(peft_model, layer_idx=0, module_name=\"q_proj\")\n",
    "    else:\n",
    "        get_model_parameter(peft_model, 0, 'v_proj')\n",
    "        print_merged_details(peft_model, layer_idx=0, module_name=\"v_proj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76973a3b-2447-441d-8bb7-face58f3b56f",
   "metadata": {},
   "source": [
    "model = PeftModel.from_pretrained(model, output_dir, torch_dtype=torch.bfloat16)\n",
    "该代码会对原来的模型再次增加一层base_model.model.base_model.model等等\n",
    "\n",
    "model = PeftModel.from_pretrained(model, output_dir, torch_dtype=torch.bfloat16) 这行代码的作用是从指定的 output_dir 加载一个经过 PEFT (Parameter-Efficient Fine-Tuning) 微调的模型。\n",
    "\n",
    "让我们逐一解析这段代码：\n",
    "\n",
    "PeftModel: 这是来自 PEFT 库的类，用于表示经过 PEFT 微调的模型。它通常是对原始模型的封装，并包含了微调过程中添加的额外参数和模块（例如，LoRA 模块）。\n",
    ".from_pretrained(model, output_dir, torch_dtype=torch.bfloat16): 这是 PeftModel 类的一个静态方法，用于从预训练的模型文件加载模型。\n",
    "model: 这个参数指定了原始模型，它可以是一个已经实例化的 PyTorch 模型对象，也可以是一个模型标识符（例如，Hugging Face 模型库中的模型名称）。\n",
    "output_dir: 这个参数指定了存储 PEFT 微调模型的目录路径。该目录应该包含了微调过程中保存的模型参数和其他相关文件。\n",
    "torch_dtype=torch.bfloat16: 这个参数指定了加载模型参数时使用的数据类型。 torch.bfloat16 是一种 16 位浮点数类型，可以减少内存占用和加速训练，但可能会损失一些精度。\n",
    "总的来说，这行代码的作用是：\n",
    "\n",
    "加载原始模型: 如果 model 参数是一个模型标识符，则会先从 Hugging Face 模型库或其他来源下载并加载原始模型。\n",
    "加载 PEFT 微调参数: 从 output_dir 目录加载 PEFT 微调过程中保存的模型参数，包括 LoRA 模块的参数和其他相关信息。\n",
    "构建 PEFT 模型: 使用加载的原始模型和 PEFT 参数，构建一个 PeftModel 对象，该对象封装了原始模型和微调后的参数。\n",
    "使用 PeftModel.from_pretrained 方法可以方便地加载经过 PEFT 微调的模型，而无需重新执行整个微调过程，从而节省时间和计算资源。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7e2d7-0fa5-44c7-a1c8-3bc26fa24006",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
